{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the libs we are going to use:\n",
    "\n",
    "* The standard [math](https://docs.python.org/3/library/math.html) module provides access to the mathematical functions.\n",
    "* The [NumPy](https://numpy.org/) lib is fundamental for any kind of scientific computing with Python.\n",
    "* [pandas](https://pandas.pydata.org/) is a must-have tool for data analysis and manipulation.\n",
    "* [matplotlib](https://matplotlib.org/) is the most complete package in Python when it comes to data visualizations.\n",
    "* [seaborn](https://seaborn.pydata.org/) is based on matplotlib as a higher-level set of visualization tools, not as powerful as matplotlib, but much easier to work with and delivers a lot with less work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have tabular data, we are going to use _pandas_ to load the data and take a first look at it.\n",
    "\n",
    "To load the data, since the format is CSV (Comma-Separated Values), we use the `read_csv()` function from pandas.\n",
    "\n",
    "Then we print its shape, which is 1168x81, meaning we have 1168 rows (records) and 81 columns (features).\n",
    "\n",
    "Actually, we have 1169 rows in the CSV file, but the header that describes the columns doesn't count.\n",
    "\n",
    "And we actually have 79 features since one of the columns is `SalePrice`, which is the column we will try to predict in a model, and we also will not use the column `Id` and will get rid of it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('raw_data.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I recommend you to read [this brief description of each column](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data).\n",
    "\n",
    "Using the `head()` function from pandas with an argument of 3, we can take a look at the first 3 records.\n",
    "\n",
    "The `.T` means _Transpose_, this way we visualize rows as columns and vice-versa.\n",
    "\n",
    "Notice how it doesn't show all of the columns in the middle and only displays `...` because there are too many of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `info()` method from pandas will give you a summary of the data.\n",
    "\n",
    "Notice how `Alley` has 70 non-null values, meaning it doesn't have a value for most of the 1168 records.\n",
    "\n",
    "We can also visualize the data types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` method is good to have the first insights of the data.\n",
    "\n",
    "It automatically gives you descriptive statistics for each feature: number of non-NA/null observations, _mean_, _standard deviation_, the _min_ value, the _quartiles_, and the _max_ value.\n",
    "\n",
    "Note that the calculations don't take `NaN` values into consideration.\n",
    "\n",
    "For `LotFrontage`, for instance, it uses only the 964 non-null values, and excludes the other 204 null observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "In this section, we will perform some Data Cleaning.\n",
    "\n",
    "### The `id` column\n",
    "\n",
    "The `id` column is only a dumb identification with no correlation to `SalePrice`.\n",
    "\n",
    "So let's remove the `id`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['Id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "When we used `info()` to see the data summary, we could see many columns had a bunch of missing data.\n",
    "\n",
    "Let's see which columns have missing values and the proportion in each one of them.\n",
    "\n",
    "`isna()` from pandas will return the missing values for each column, then the `sum()` function will add them up to give you a total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_miss = train.isna().sum()\n",
    "#filtering only the columns with at least 1 missing value\n",
    "columns_with_miss = columns_with_miss[columns_with_miss!=0]\n",
    "#The number of columns with missing values\n",
    "print('Columns with missing values:', len(columns_with_miss))\n",
    "#sorting the columns by the number of missing values descending\n",
    "columns_with_miss.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 80 columns, 19 have missing values. \n",
    "\n",
    "Missing values per se it not a big problem, but columns with a high number of missing values can cause distortions.\n",
    "\n",
    "This is the case for:\n",
    "\n",
    "* PoolQC: Pool quality\n",
    "* MiscFeature: Miscellaneous feature not covered in other categories\n",
    "* Alley: Type of alley access to property\n",
    "* Fence: Fence quality\n",
    "\n",
    "Let's drop them from the dataset for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing columns\n",
    "train.drop(columns=['PoolQC', 'MiscFeature', 'Alley', 'Fence'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FireplaceQu has 551 missing values, which is also pretty high.\n",
    "\n",
    "In this case, the missing values have meaning, which is \"NO Fireplace\".\n",
    "\n",
    "Fireplace has the following categories:\n",
    "\n",
    "* Ex Excellent - Exceptional Masonry Fireplace\n",
    "* Gd Good - Masonry Fireplace in main level\n",
    "* TA Average - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n",
    "* Fa Fair - Prefabricated Fireplace in basement\n",
    "* Po Poor - Ben Franklin Stove\n",
    "* NA No Fireplace\n",
    "\n",
    "Let's check the correlation between FireplaceQu and SalePrice, to see how important this feature is in order to determine the price.\n",
    "\n",
    "First, we will replace the missing values for 0.\n",
    "\n",
    "Then, we encode the categories into numbers from 1 to 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['FireplaceQu'].fillna(0, inplace=True)\n",
    "train['FireplaceQu'].replace({'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a barplot, we can see how the category of the FirePlace increases the value of SalePrice.\n",
    "\n",
    "It is also worth noting how much higher the value is when the house has an Excellent fireplace.\n",
    "\n",
    "This means we should keep FireplaceQu as feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "sns.barplot(x='FireplaceQu', y=\"SalePrice\", data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values in numeric columns\n",
    "\n",
    "Another feature with a high number of missing values is LotFrontage with a count 204.\n",
    "\n",
    "Letâ€™s see the correlation between the remaining features with missing values and the SalePrice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_miss = train.isna().sum()\n",
    "columns_with_miss = columns_with_miss[columns_with_miss!=0]\n",
    "c = list(columns_with_miss.index)\n",
    "c.append('SalePrice')\n",
    "train[c].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that LotFrontage, MasVnrArea, and GarageYrBlt have a positive correlation with SalePrice, but this correlation isn't very strong.\n",
    "\n",
    "To simplify this analisys, we will remove theses columns for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_be_removed = ['LotFrontage', 'GarageYrBlt', 'MasVnrArea']\n",
    "train.drop(columns=cols_to_be_removed, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, these are the remaining columns with missing values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_miss = train.isna().sum()\n",
    "columns_with_miss = columns_with_miss[columns_with_miss!=0]\n",
    "print(f'Columns with missing values: {len(columns_with_miss)}')\n",
    "columns_with_miss.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical variables\n",
    "\n",
    "Let's work on the categorical variables of our dataset.\n",
    "\n",
    "### Dealing with missing values\n",
    "\n",
    "Filling Categorical NaN that we know how to fill due to the [description file](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data?select=data_description.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fills NA in place of NaN\n",
    "for c in ['GarageType', 'GarageFinish', 'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1']:\n",
    "    train[c].fillna('NA', inplace=True)\n",
    "    \n",
    "# Fills None in place of NaN\n",
    "train['MasVnrType'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this have only 5 columns with missing values left in our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_miss = train.isna().sum()\n",
    "columns_with_miss = columns_with_miss[columns_with_miss!=0]\n",
    "print(f'Columns with missing values: {len(columns_with_miss)}')\n",
    "columns_with_miss.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal\n",
    "\n",
    "Also by reading the [description file](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data?select=data_description.txt), we can identify other variables that have a similar system to FireplaceQu to categorize the quality: Poor, Good, Excellent, etc.\n",
    "\n",
    "We are going to replicate the treatment we gave to FireplaceQu to these variables according to the following descriptions:\n",
    "\n",
    "ExterQual: Evaluates the quality of the material on the exterior\n",
    "\n",
    "* Ex Excellent\n",
    "* Gd Good\n",
    "* TA Average/Typical\n",
    "* Fa Fair\n",
    "* Po Poor\n",
    "\n",
    "ExterCond: Evaluates the present condition of the material on the exterior\n",
    "\n",
    "* Ex Excellent\n",
    "* Gd Good\n",
    "* TA Average/Typical\n",
    "* Fa Fair\n",
    "* Po Poor\n",
    "\n",
    "BsmtQual: Evaluates the height of the basement\n",
    "\n",
    "* Ex Excellent (100+ inches)\n",
    "* Gd Good (90-99 inches)\n",
    "* TA Typical (80-89 inches)\n",
    "* Fa Fair (70-79 inches)\n",
    "* Po Poor ( < 70 inches)\n",
    "* NA No Basement\n",
    "\n",
    "BsmtCond: Evaluates the general condition of the basement\n",
    "\n",
    "* Ex Excellent\n",
    "* Gd Good\n",
    "* TA Typical - slight dampness allowed\n",
    "* Fa Fair - dampness or some cracking or settling\n",
    "* Po Poor - Severe cracking, settling, or wetness\n",
    "* NA No Basement\n",
    "\n",
    "HeatingQC: Heating quality and condition\n",
    "\n",
    "* Ex Excellent\n",
    "* Gd Good\n",
    "* TA Average/Typical\n",
    "* Fa Fair\n",
    "* Po Poor\n",
    "\n",
    "KitchenQual: Kitchen quality\n",
    "\n",
    "* Ex Excellent\n",
    "* Gd Good\n",
    "* TA Average/Typical\n",
    "* Fa Fair\n",
    "* Po Poor\n",
    "\n",
    "GarageQual: Garage quality\n",
    "\n",
    "* Ex Excellent\n",
    "* Gd Good\n",
    "* TA Average/Typical\n",
    "* Fa Fair\n",
    "* Po Poor\n",
    "* NA No Garage\n",
    "\n",
    "GarageCond: Garage condition\n",
    "\n",
    "* Ex Excellent\n",
    "* Gd Good\n",
    "* TA Average/Typical\n",
    "* Fa Fair\n",
    "* Po Poor\n",
    "* NA No Garage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'GarageQual', 'GarageCond']\n",
    "for col in ord_cols:\n",
    "    train[col].fillna(0, inplace=True)\n",
    "    train[col].replace({'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot the correlation of these variables with SalePrice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC', 'KitchenQual', 'GarageQual', 'GarageCond']\n",
    "f, axes = plt.subplots(2, 4, figsize=(15, 10), sharey=True)\n",
    "\n",
    "for r in range(0, 2):\n",
    "    for c in range(0, 4):\n",
    "        sns.barplot(x=ord_cols.pop(), y=\"SalePrice\", data=train, ax=axes[r][c])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the better the category of a variable, the higher the price, which means these variables will be important for a prediction model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nominal\n",
    "\n",
    "Other categorical variables don't seem to follow any clear ordering.\n",
    "\n",
    "Let's see how many values these columns can assume:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train.columns\n",
    "num_cols = train._get_numeric_data().columns\n",
    "nom_cols = list(set(cols) - set(num_cols))\n",
    "print(f'Nominal columns: {len(nom_cols)}')\n",
    "\n",
    "value_counts = {}\n",
    "for c in nom_cols:\n",
    "    value_counts[c] = len(train[c].value_counts())\n",
    "\n",
    "sorted_value_counts = {k: v for k, v in sorted(value_counts.items(), key=lambda item: item[1])}\n",
    "sorted_value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some categorical variables can assume several different values like Neighborhood. \n",
    "\n",
    "To simplify, let's analyze only variables with 6 different values or less.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nom_cols_less_than_6 = []\n",
    "for c in nom_cols:\n",
    "    n_values = len(train[c].value_counts())\n",
    "    if n_values < 7:\n",
    "        nom_cols_less_than_6.append(c)\n",
    "\n",
    "print(f'Nominal columns with less than 6 values: {len(nom_cols_less_than_6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Plotting against SalePrice to have a better idea of how they affect it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = math.ceil(len(nom_cols_less_than_6) / ncols)\n",
    "f, axes = plt.subplots(nrows, ncols, figsize=(15, 30))\n",
    "\n",
    "for r in range(0, nrows):\n",
    "    for c in range(0, ncols):\n",
    "        if not nom_cols_less_than_6:\n",
    "            continue\n",
    "        sns.barplot(x=nom_cols_less_than_6.pop(), y=\"SalePrice\", data=train, ax=axes[r][c])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a good correlation of many of these columns with the target variable.\n",
    "\n",
    "For now, let's keep them.\n",
    "\n",
    "We still have NaN in 'Electrical'.\n",
    "\n",
    "As we could see in the plot above, 'SBrkr' is the most frequent value in 'Electrical'.\n",
    "\n",
    "Let's use this value to replace NaN in Electrical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs more frequent value in place of NaN\n",
    "\n",
    "train['Electrical'].fillna('SBrkr', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Zero values\n",
    "\n",
    "Another quick check is to see how many columns have lots of data equals to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isin([0]).sum().sort_values(ascending=False).head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, even though there are many 0's, they have meaning.\n",
    "\n",
    "For instance, PoolArea (Pool area in square feet) equals 0 means that the house doesn't have any pool area.\n",
    "\n",
    "This is important information correlated to the house and thus, we are going to keep them.\n",
    "\n",
    "## Saving cleaned data\n",
    "\n",
    "Let's see how the cleaned data looks like and how many columns we have left.\n",
    "\n",
    "We have no more missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_miss = train.isna().sum()\n",
    "columns_with_miss = columns_with_miss[columns_with_miss!=0]\n",
    "print(f'Columns with missing values: {len(columns_with_miss)}')\n",
    "columns_with_miss.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning the data, we are left with 73 columns out of the initial 81.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first 3 records of the cleaned data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a summary of the data showing that, for all the 1168 records, there isn't a single missing (null) value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the cleaned data in a separate file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train-cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We dealt with missing values and removed the following columns: 'Id', 'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'LotFrontage', 'GarageYrBlt', 'MasVnrArea'.\n",
    "\n",
    "We also:\n",
    "\n",
    "* Replaced the NaN with NA in the following columns: 'GarageType', 'GarageFinish', 'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1'.\n",
    "* Replaced the NaN with None in 'MasVnrType'.\n",
    "* Imputed the most frequent value in place of NaN in 'Electrical'.\n",
    "\n",
    "Please note that the removed columns are not useless and may contribute to the final model.\n",
    "\n",
    "After the first round of analysis and testing of the hypothesis, if you ever need to improve your future model further, you can consider reevaluating these columns and understand them better to see how they fit into the problem.\n",
    "\n",
    "Data Analysis and Machine Learning is NOT a straight path.\n",
    "\n",
    "It is a process where you iterate and keep testing ideas until you have the result you want, or until find out the result you need is not possible.\n",
    "\n",
    "We are going to use this data to create our Machine Learning model and predict the house prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
